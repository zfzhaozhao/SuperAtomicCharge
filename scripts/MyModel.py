"""
some codes in this script was based on
https:https://github.com/awslabs/dgl-lifesci
"""

import torch.nn as nn
from dgllife.model.gnn import GAT  #å›¾æ³¨æ„åŠ›ç½‘ç»œ
import dgl.function as fn
import torch
import torch.nn as nn
import torch.nn.functional as F
from dgl.nn.pytorch import edge_softmax


class AttentiveGRU1(nn.Module):  ##å‘èŠ‚ç‚¹ä¼ é€’è¾¹ä¿¡æ¯
    def __init__(self, node_feat_size, edge_feat_size, edge_hidden_size, dropout): #ä¸‰ä¸ªç»´åº¦ï¼ŒèŠ‚ç‚¹ç»´åº¦ï¼Œè¾¹ç»´åº¦ï¼Œéšè—å±‚ç»´åº¦
        super(AttentiveGRU1, self).__init__()

        self.edge_transform = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(edge_feat_size, edge_hidden_size)
        )  #è¾¹ç»´åº¦è½¬éšè—å±‚ç»´åº¦
        self.gru = nn.GRUCell(edge_hidden_size, node_feat_size) #ä¸å®Œæ•´çš„ nn.GRU ä¸åŒï¼Œnn.GRUCell åªå®ç°äº† GRU çš„å•ä¸ªæ—¶é—´æ­¥è®¡ç®—ï¼Œè€Œä¸å¤„ç†æ•´ä¸ªåºåˆ—ã€‚å®ƒé€‚åˆäºéœ€è¦å¯¹æ¯ä¸ªæ—¶é—´æ­¥å•ç‹¬è¿›è¡Œå¤„ç†çš„æƒ…å†µ
#è¾¹ç»´åº¦å˜åŒ–ï¼šè¾¹åŸå§‹ç»´åº¦-éšè—å±‚-èŠ‚ç‚¹ç»´åº¦
    
    def forward(self, g, edge_logits, edge_feats, node_feats): #è¿™ä¸ª edge_logits æ˜¯ä»€ä¹ˆï¼Ÿï¼Ÿï¼Ÿåé¢çš„å‡½æ•°ä¸­ï¼Œä¼šè®¡ç®—è¿™ä¸ªå€¼ï¼Œä½†æ˜¯æˆ‘è¿˜æ˜¯ğŸ˜µ
#logitsï¼ˆé€šå¸¸æ˜¯æŸç§å½¢å¼çš„å¾—åˆ†æˆ–æœªå½’ä¸€åŒ–çš„æ¦‚ç‡ï¼ˆçŒœæµ‹ï¼‰
      
        g = g.local_var()
#.local_var():è¿™ä¸ªæ–¹æ³•ç”¨äºåˆ›å»ºä¸€ä¸ªå›¾çš„å±€éƒ¨å‰¯æœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒè¿”å›ä¸€ä¸ªæ–°çš„å›¾å¯¹è±¡ï¼Œè¿™ä¸ªå›¾å¯¹è±¡çš„ç»“æ„å’Œæ•°æ®ä¸åŸå§‹å›¾ g ç›¸åŒï¼Œä½†å®ƒæ˜¯ä¸€ä¸ªå±€éƒ¨å‰¯æœ¬ï¼Œé€šå¸¸ç”¨äºåœ¨è®¡ç®—è¿‡ç¨‹ä¸­è¿›è¡Œä¸´æ—¶æ“ä½œã€‚
#ä½¿ç”¨ local_var() å¯ä»¥ç¡®ä¿å¯¹å›¾çš„ä¿®æ”¹ä¸ä¼šå½±å“åˆ°åŸå§‹å›¾ gï¼Œè¿™å¯¹äºå›¾è®¡ç®—ä¸­çš„ä¸­é—´æ­¥éª¤ç‰¹åˆ«æœ‰ç”¨ã€‚
        
        g.edata['e'] = edge_softmax(g, edge_logits) * self.edge_transform(edge_feats)  #è¿™é‡Œç»´åº¦éœ€è¦æ³¨æ„ä¸€ä¸‹ï¼Œè¿™ä¸ªlogistçš„ç»´åº¦å¤§å°ï¼Ÿ ä¸ç„¶ä¸å¯ä»¥è¿ç®—
#edge_softmax çš„ç›®çš„æ˜¯å¯¹æ¯æ¡è¾¹çš„ logits åº”ç”¨ softmax å‡½æ•°ï¼Œä½¿å¾—æ¯æ¡è¾¹çš„å¾—åˆ†è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯æ¡è¾¹ï¼Œsoftmax å‡½æ•°å°†å¾—åˆ†è½¬æ¢ä¸º 0 åˆ° 1 ä¹‹é—´çš„å€¼ï¼Œå¹¶ä¸”æ‰€æœ‰è¾¹çš„æ¦‚ç‡å’Œä¸º 1ã€‚
     
        g.update_all(fn.copy_edge('e', 'm'), fn.sum('m', 'c'))  
#é‚»å±…è¾¹æ˜¯æŒ‡ä¸å½“å‰èŠ‚ç‚¹ç›¸è¿çš„è¾¹ã€‚DGL çš„æ¶ˆæ¯ä¼ é€’æœºåˆ¶ä¼šè‡ªåŠ¨å¤„ç†è¿™äº›è¾¹ã€‚å…·ä½“æ¥è¯´ï¼š
#è¾¹çš„èšåˆ: å½“ä½ è°ƒç”¨ update_all æ—¶ï¼ŒDGL ä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰ä¸æ¯ä¸ªèŠ‚ç‚¹ç›¸è¿çš„è¾¹ã€‚å¯¹äºæ¯ä¸ªèŠ‚ç‚¹ï¼ŒDGL ä¼šæ”¶é›†æ‰€æœ‰ä»è¯¥èŠ‚ç‚¹å‡ºå‘çš„è¾¹çš„æ¶ˆæ¯ï¼Œå¹¶æŒ‰ç…§ä½ æŒ‡å®šçš„èšåˆå‡½æ•°å¯¹è¿™äº›æ¶ˆæ¯è¿›è¡Œå¤„ç†ã€‚
#åœ¨ DGL ä¸­ï¼Œg.update_all æ–¹æ³•ç”¨äºå¯¹èŠ‚ç‚¹è¿›è¡Œæ¶ˆæ¯ä¼ é€’å’Œä¿¡æ¯èšåˆï¼Œä½†å®ƒä¸ä¼šç›´æ¥è¦†ç›–åŸå§‹çš„è¾¹ç‰¹å¾ã€‚å…·ä½“åœ°è¯´ï¼Œg.update_all æ–¹æ³•ä¸­çš„æ¶ˆæ¯ä¼ é€’å’Œèšåˆæ“ä½œæ˜¯å°†ä¿¡æ¯ä¼ é€’åˆ°èŠ‚ç‚¹ä¸Šï¼Œå¹¶å°†ç»“æœå­˜å‚¨åœ¨èŠ‚ç‚¹ç‰¹å¾ä¸­ã€‚åŸå§‹çš„è¾¹ç‰¹å¾åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ä¸ä¼šè¢«ä¿®æ”¹ã€‚

#update_all æ˜¯ DGL å›¾å¯¹è±¡ä¸­çš„ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºåœ¨å›¾çš„æ‰€æœ‰èŠ‚ç‚¹ä¸Šæ‰§è¡Œæ¶ˆæ¯ä¼ é€’æ“ä½œã€‚åœ¨è¿™ä¸ªæ–¹æ³•ä¸­ï¼Œé€šå¸¸ä¼šæŒ‡å®šä¸€ä¸ªæ¶ˆæ¯ä¼ é€’å‡½æ•°ï¼ˆmessage_funcï¼‰å’Œä¸€ä¸ªèšåˆå‡½æ•°ï¼ˆreduce_funcï¼‰
#fn.copy_edge('e', 'm'):è¿™æ˜¯ä¸€ä¸ªæ¶ˆæ¯ä¼ é€’å‡½æ•°ï¼ˆmessage_funcï¼‰ï¼Œå®ƒå®šä¹‰äº†åœ¨å›¾ä¸­ä¼ é€’æ¶ˆæ¯çš„æ–¹å¼ã€‚fn.copy_edge æ˜¯ä¸€ä¸ª DGL æä¾›çš„å†…ç½®å‡½æ•°ï¼Œç”¨äºå°†è¾¹ä¸Šçš„ç‰¹å¾ä»æºèŠ‚ç‚¹å¤åˆ¶åˆ°æ¶ˆæ¯ä¸­ã€‚
#'e' æ˜¯è¾¹ç‰¹å¾çš„é”®ï¼Œè¡¨ç¤ºè¦å¤åˆ¶çš„è¾¹ç‰¹å¾ã€‚
#'m' æ˜¯æ¶ˆæ¯çš„é”®ï¼Œè¡¨ç¤ºæ¶ˆæ¯çš„å†…å®¹å°†è¢«å­˜å‚¨åˆ° 'm' ä¸­ã€‚
#è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯å°†æ¯æ¡è¾¹çš„ç‰¹å¾ 'e' å¤åˆ¶åˆ°æ¶ˆæ¯ 'm' ä¸­ï¼Œæ¶ˆæ¯ 'm' å°†åœ¨æ¥ä¸‹æ¥çš„æ­¥éª¤ä¸­è¢«ç”¨äºèšåˆã€‚

#fn.sum('m', 'c'):è¿™æ˜¯ä¸€ä¸ªèšåˆå‡½æ•°ï¼ˆreduce_funcï¼‰ï¼Œå®ƒå®šä¹‰äº†å¦‚ä½•å°†æ¶ˆæ¯èšåˆåˆ°ç›®æ ‡èŠ‚ç‚¹ä¸Šã€‚fn.sum æ˜¯ DGL æä¾›çš„å†…ç½®å‡½æ•°ï¼Œç”¨äºå¯¹ä¼ é€’åˆ°ç›®æ ‡èŠ‚ç‚¹çš„æ‰€æœ‰æ¶ˆæ¯è¿›è¡Œæ±‚å’Œæ“ä½œã€‚
#'m' æ˜¯æ¶ˆæ¯çš„é”®ï¼Œè¡¨ç¤ºè¦è¿›è¡Œèšåˆçš„æ¶ˆæ¯ã€‚
#'c' æ˜¯èšåˆç»“æœçš„é”®ï¼Œè¡¨ç¤ºå°†èšåˆç»“æœå­˜å‚¨åœ¨ 'c' ä¸­ã€‚
#è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯å°†ä»æ‰€æœ‰é‚»å±…èŠ‚ç‚¹ä¼ é€’è¿‡æ¥çš„æ¶ˆæ¯ 'm' è¿›è¡Œæ±‚å’Œï¼Œå¹¶å°†ç»“æœå­˜å‚¨åœ¨ç›®æ ‡èŠ‚ç‚¹çš„ç‰¹å¾ 'c' ä¸­ã€‚
        
        context = F.elu(g.ndata['c'])
        return F.relu(self.gru(context, node_feats))
#å¾—åˆ°æ˜¯è·å–äº†è¾¹ç‰¹å¾çš„èŠ‚ç‚¹ä¿¡æ¯ï¼Œè¿™äº›ç‰¹å¾ç»è¿‡ GRU å•å…ƒå¤„ç†å¹¶é€šè¿‡ ReLU æ¿€æ´»å‡½æ•°è¿›è¡Œéçº¿æ€§å˜æ¢ã€‚ï¼ˆç»´åº¦åº”è¯¥æ˜¯ï¼ˆèŠ‚ç‚¹æ•°é‡ï¼ŒèŠ‚ç‚¹ç‰¹å¾ç»´åº¦ï¼ˆç»gruè½¬çš„ï¼Œå‰é¢transformçš„å‡½æ•°å°±å°†è¾¹ç»´åº¦è½¬éšè—å±‚ç»´åº¦äº†ï¼‰

class AttentiveGRU2(nn.Module): #ä¼ é€’èŠ‚ç‚¹ä¿¡æ¯
    def __init__(self, node_feat_size, edge_hidden_size, dropout):
        super(AttentiveGRU2, self).__init__()

        self.project_node = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(node_feat_size, edge_hidden_size)
        )
        self.gru = nn.GRUCell(edge_hidden_size, node_feat_size)

    def forward(self, g, edge_logits, node_feats):
        g = g.local_var()
        g.edata['a'] = edge_softmax(g, edge_logits) #g.edata: è¿™æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºå­˜å‚¨å›¾ä¸­è¾¹çš„æ•°æ®ç‰¹å¾ã€‚
        g.ndata['hv'] = self.project_node(node_feats) #g.ndata: è¿™æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºå­˜å‚¨å›¾ä¸­èŠ‚ç‚¹çš„æ•°æ®ç‰¹å¾ã€‚

        g.update_all(fn.src_mul_edge('hv', 'a', 'm'), fn.sum('m', 'c'))  #å…³æ³¨ç»´åº¦é—®é¢˜
        
#fn.src_mul_edge: è¿™æ˜¯ DGL ä¸­çš„ä¸€ä¸ªæ¶ˆæ¯å‡½æ•°ï¼Œç”¨äºè®¡ç®—æ¯æ¡è¾¹çš„æ¶ˆæ¯ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå®ƒæ‰§è¡ŒæºèŠ‚ç‚¹ç‰¹å¾å’Œè¾¹ç‰¹å¾çš„ä¹˜æ³•ã€‚
#å‚æ•°è§£é‡Š:'hv': è¿™æ˜¯æºèŠ‚ç‚¹çš„ç‰¹å¾åç§°ã€‚'a': è¿™æ˜¯è¾¹ç‰¹å¾çš„åç§°ã€‚'m': è¿™æ˜¯è®¡ç®—å‡ºçš„æ¶ˆæ¯çš„å­˜å‚¨é”®åã€‚
#å¯¹äºæ¯æ¡è¾¹ï¼Œfn.src_mul_edge('hv', 'a', 'm') ä¼šå°†æºèŠ‚ç‚¹çš„ 'hv' ç‰¹å¾ä¸è¾¹çš„ 'a' ç‰¹å¾é€å…ƒç´ ç›¸ä¹˜ï¼Œå¾—åˆ°æ¶ˆæ¯ 'm'ã€‚
#  fn.sum: è¿™æ˜¯ DGL ä¸­çš„ä¸€ä¸ªèšåˆå‡½æ•°ï¼Œç”¨äºå¯¹è¾¹å‘æ¥çš„æ¶ˆæ¯è¿›è¡Œèšåˆã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå®ƒå¯¹æ¶ˆæ¯è¿›è¡Œæ±‚å’Œã€‚
#å‚æ•°è§£é‡Š:'m': è¿™æ˜¯ä»æ¶ˆæ¯å‡½æ•°ä¸­ä¼ é€’è¿‡æ¥çš„æ¶ˆæ¯åç§°ã€‚'c': è¿™æ˜¯èšåˆç»“æœçš„å­˜å‚¨é”®åã€‚
        context = F.elu(g.ndata['c'])
        return F.relu(self.gru(context, node_feats))


class GetContext(nn.Module):
    def __init__(self, node_feat_size, edge_feat_size, graph_feat_size, dropout): 
        super(GetContext, self).__init__()

        self.project_node = nn.Sequential(
            nn.Linear(node_feat_size, graph_feat_size),  #å›¾ç‰¹å¾å¤§å°ï¼Ÿï¼Ÿ
            nn.LeakyReLU()
        )
        self.project_edge1 = nn.Sequential(
            nn.Linear(node_feat_size + edge_feat_size, graph_feat_size),
            nn.LeakyReLU()
        )
        self.project_edge2 = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(2 * graph_feat_size, 1),
            nn.LeakyReLU()
        )
        self.attentive_gru = AttentiveGRU1(graph_feat_size, graph_feat_size, 
                                           graph_feat_size, dropout)  ##node_feat_size, edge_feat_size, edge_hidden_size ï¼ˆè¿™ä¸ªç»´åº¦ï¼Ÿï¼Ÿï¼Ÿ

    def apply_edges1(self, edges):
        return {'he1': torch.cat([edges.src['hv'], edges.data['he']], dim=1)}
    #{'he1': torch.cat([edges.src['hv'], edges.data['he']], dim=1)} æ˜¯åœ¨ DGLï¼ˆDeep Graph Libraryï¼‰ä¸­çš„æ¶ˆæ¯ä¼ é€’å‡½æ•°ä¸­ï¼Œ
    #å®šä¹‰äº†å¦‚ä½•æ„é€ æ¶ˆæ¯ã€‚å®ƒçš„ä½œç”¨æ˜¯å°†æºèŠ‚ç‚¹çš„ç‰¹å¾å’Œè¾¹ç‰¹å¾ç»“åˆèµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªæ–°çš„æ¶ˆæ¯ã€‚
    #dim=1 æŒ‡å®šäº†æ²¿ç€ç‰¹å¾ç»´åº¦è¿›è¡Œæ‹¼æ¥ã€‚å‡è®¾æºèŠ‚ç‚¹ç‰¹å¾çš„ç»´åº¦æ˜¯ feature_dimï¼Œè¾¹ç‰¹å¾çš„ç»´åº¦æ˜¯ edge_feature_dimï¼Œæ‹¼æ¥åçš„æ¶ˆæ¯çš„ç»´åº¦å°†æ˜¯ feature_dim + edge_feature_dim
    def apply_edges2(self, edges):
        return {'he2': torch.cat([edges.dst['hv_new'], edges.data['he1']], dim=1)}

    def forward(self, g, node_feats, edge_feats):
        g = g.local_var()
        g.ndata['hv'] = node_feats   #ï¼ˆèŠ‚ç‚¹æ•°é‡ï¼ŒèŠ‚ç‚¹ç‰¹å¾ï¼‰
        g.ndata['hv_new'] = self.project_node(node_feats)  #ï¼ˆèŠ‚ç‚¹æ•°é‡ï¼Œå›¾ç‰¹å¾)
        g.edata['he'] = edge_feats  #(è¾¹æ•°é‡ï¼Œè¾¹ç‰¹å¾ï¼‰

        g.apply_edges(self.apply_edges1)   #ï¼ˆè¾¹æ•°é‡ï¼ŒèŠ‚ç‚¹ç‰¹å¾+è¾¹ç‰¹å¾ï¼‰
        #apply_edges æ˜¯ DGL æä¾›çš„ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºåœ¨å›¾çš„è¾¹ä¸Šåº”ç”¨æŸä¸ªæ“ä½œã€‚
        #å®ƒæ¥å—ä¸€ä¸ªå‡½æ•°ä½œä¸ºå‚æ•°ï¼Œè¿™ä¸ªå‡½æ•°ä¼šè¢«åº”ç”¨åˆ°æ¯æ¡è¾¹ä¸Šã€‚

        g.edata['he1'] = self.project_edge1(g.edata['he1']) #ï¼ˆè¾¹æ•°é‡ï¼Œå›¾ç‰¹å¾ï¼‰
        g.apply_edges(self.apply_edges2)   #ï¼ˆï¼ˆè¾¹æ•°é‡ï¼Œ2*å›¾ç‰¹å¾ï¼‰
        logits = self.project_edge2(g.edata['he2']) #ï¼ˆè¾¹æ•°é‡ï¼Œ1ï¼‰

        return self.attentive_gru(g, logits, g.edata['he1'], g.ndata['hv_new'])  #æ‰€ä»¥ä¸Šé¢çš„ç»´åº¦å…¨æ˜¯å›¾ç‰¹å¾


class GNNLayer(nn.Module):
    def __init__(self, node_feat_size, graph_feat_size, dropout):
        super(GNNLayer, self).__init__()

        self.project_edge = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(2 * node_feat_size, 1),
            nn.LeakyReLU()
        )
        self.attentive_gru = AttentiveGRU2(node_feat_size, graph_feat_size, dropout)  #å¤„ç†èŠ‚ç‚¹ä¿¡æ¯ä¼ é€’  #(self, node_feat_size, edge_hidden_size, dropout):
        self.bn_layer = nn.BatchNorm1d(graph_feat_size)

    def apply_edges(self, edges):
        return {'he': torch.cat([edges.dst['hv'], edges.src['hv']], dim=1)}

    def forward(self, g, node_feats):
        g = g.local_var()
        g.ndata['hv'] = node_feats        #ï¼ˆèŠ‚ç‚¹æ•°é‡ï¼ŒèŠ‚ç‚¹ç‰¹å¾ï¼‰
        g.apply_edges(self.apply_edges)   #ï¼ˆè¾¹æ•°é‡ï¼Œ2*èŠ‚ç‚¹ç‰¹å¾ï¼‰
        logits = self.project_edge(g.edata['he'])  #ï¼ˆè¾¹æ•°é‡ï¼Œ1ï¼‰

        return self.bn_layer(self.attentive_gru(g, logits, node_feats))


class ModifiedChargeModelNNV2(nn.Module):
    def __init__(self,
                 node_feat_size,
                 edge_feat_size,
                 num_layers=2,
                 graph_feat_size=200,
                 dropout=0.):
        super(ModifiedChargeModelNNV2, self).__init__()

        self.init_context = GetContext(node_feat_size, edge_feat_size, graph_feat_size, dropout) #å¾—åˆ°çš„æ˜¯ä¼ é€’è¾¹çš„èŠ‚ç‚¹ä¿¡æ¯ï¼ˆæ­¤æ—¶çš„ç»´åº¦èŠ‚ç‚¹æ•°é‡ï¼Œå›¾ç‰¹å¾ï¼‰
        self.gnn_layers = nn.ModuleList()
        self.sum_node_feats = 0
        for _ in range(num_layers - 1):
            self.gnn_layers.append(GNNLayer(graph_feat_size, graph_feat_size, dropout))  #èŠ‚ç‚¹ä¿¡æ¯ä¼ é€’

    def forward(self, g, node_feats, edge_feats):
        node_feats = self.init_context(g, node_feats, edge_feats)
        self.sum_node_feats = node_feats
        for gnn in self.gnn_layers:
            node_feats = gnn(g, node_feats)
            self.sum_node_feats = self.sum_node_feats + node_feats  #å¯ä»¥åŠ ï¼Œç»´åº¦ä¸€è‡´
        return self.sum_node_feats


class ModifiedChargeModelV2(nn.Module):  #åŠ äº†é¢„æµ‹ï¼ˆç›¸å½“ä¸å…¨è¿æ¥å±‚ï¼‰
    def __init__(self,
                 node_feat_size,
                 edge_feat_size,
                 num_layers=2,
                 graph_feat_size=200,
                 dropout=0.):
        super(ModifiedChargeModelV2, self).__init__()

        self.gnn = ModifiedChargeModelNNV2(node_feat_size=node_feat_size,
                                           edge_feat_size=edge_feat_size,
                                           num_layers=num_layers,
                                           graph_feat_size=graph_feat_size,
                                           dropout=dropout)
        self.predict = nn.Sequential(
            nn.Linear(graph_feat_size, graph_feat_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(graph_feat_size, 1)
        )

    def forward(self, g, node_feats, edge_feats):
        sum_node_feats = self.gnn(g, node_feats, edge_feats)
        return self.predict(sum_node_feats)


class ModifiedChargeModelV2New(nn.Module):  #é¢„æµ‹æ˜¯ï¼Œç”¨äºå¤šé¡¹ä»»åŠ¡çš„
    def __init__(self,
                 node_feat_size,
                 edge_feat_size,
                 num_layers=2,
                 graph_feat_size=200,
                 dropout=0., n_tasks=1):
        super(ModifiedChargeModelV2New, self).__init__()

        self.gnn = ModifiedChargeModelNNV2(node_feat_size=node_feat_size,
                                           edge_feat_size=edge_feat_size,
                                           num_layers=num_layers,
                                           graph_feat_size=graph_feat_size,
                                           dropout=dropout)
        self.predict = nn.Sequential(
            nn.Linear(graph_feat_size, graph_feat_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(graph_feat_size, n_tasks)
        )

    def forward(self, g, node_feats, edge_feats):
        sum_node_feats = self.gnn(g, node_feats, edge_feats)
        return self.predict(sum_node_feats)


class ModifiedChargeModelNNV3(nn.Module): #ä¸å¤ªæ˜ç™½ï¼Œä¸€æ¬¡gnnå°±é¢„æµ‹ä¸€æ¬¡ï¼Œæœ€åæ˜¯é¢„æµ‹æ±‚å’Œ/å±‚æ•°ï¼Ÿï¼Ÿï¼Ÿç”¨æ„
    def __init__(self,
                 node_feat_size,
                 edge_feat_size,
                 num_layers=2,
                 graph_feat_size=200,
                 dropout=0.):
        super(ModifiedChargeModelNNV3, self).__init__()

        self.init_context = GetContext(node_feat_size, edge_feat_size, graph_feat_size, dropout)
        self.gnn_layers = nn.ModuleList()
        for _ in range(num_layers - 1):
            self.gnn_layers.append(GNNLayer(graph_feat_size, graph_feat_size, dropout))
        self.predict = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(graph_feat_size, 1)
        )
        self.sum_predictions = 0
        self.num_layers = num_layers

    def forward(self, g, node_feats, edge_feats):
        node_feats = self.init_context(g, node_feats, edge_feats)
        for gnn in self.gnn_layers:
            node_feats = gnn(g, node_feats)
            self.sum_predictions = self.sum_predictions + self.predict(node_feats)
        return self.sum_predictions / (self.num_layers - 1)


class ModifiedChargeModelV3(nn.Module):
    def __init__(self,
                 node_feat_size,
                 edge_feat_size,
                 num_layers=2,
                 graph_feat_size=200,
                 dropout=0.):
        super(ModifiedChargeModelV3, self).__init__()

        self.gnn = ModifiedChargeModelNNV3(node_feat_size=node_feat_size,
                                            edge_feat_size=edge_feat_size,
                                            num_layers=num_layers,
                                            graph_feat_size=graph_feat_size,
                                            dropout=dropout)

    def forward(self, g, node_feats, edge_feats):
        predictions = self.gnn(g, node_feats, edge_feats)
        return predictions


class ModifiedGATPredictor(nn.Module): #ç”¨æ¥é¢„æµ‹çš„  å•Šï¼Ÿï¼Ÿé¢„æµ‹çš„æ¨¡å‹æ˜¯GAT é‚£ä½ å‰é¢è®­ç»ƒä¸ªé¬¼å•Šï¼Œæ¨¡å‹éƒ½ä¸ä¸€æ ·ï¼Œç¦»è°±
    def __init__(self, in_feats, hidden_feats=None, num_heads=None, feat_drops=None,
                 attn_drops=None, alphas=None, residuals=None, agg_modes=None, activations=None):
        super(ModifiedGATPredictor, self).__init__()

        self.gnn = GAT(in_feats=in_feats,
                       hidden_feats=hidden_feats,
                       num_heads=num_heads,
                       feat_drops=feat_drops,
                       attn_drops=attn_drops,
                       alphas=alphas,
                       residuals=residuals,
                       agg_modes=agg_modes,
                       activations=activations)
#æˆ‘å…¶å®ä¸ç†è§£è¿™é‡Œ
        if self.gnn.agg_modes[-1] == 'flatten':
            gnn_out_feats = self.gnn.hidden_feats[-1] * self.gnn.num_heads[-1]
        else:
            gnn_out_feats = self.gnn.hidden_feats[-1]
        self.predict = nn.Sequential(nn.Linear(gnn_out_feats, 1))
                     
#self.gnn.agg_modes[-1]:
#self.gnn.agg_modes æ˜¯ä¸€ä¸ªåŒ…å«èšåˆæ¨¡å¼çš„åˆ—è¡¨ï¼Œ[-1] å–çš„æ˜¯åˆ—è¡¨çš„æœ€åä¸€ä¸ªå…ƒç´ ï¼Œå³å½“å‰å±‚çš„èšåˆæ¨¡å¼ã€‚
#è¿™ä¸ªèšåˆæ¨¡å¼å†³å®šäº†å¦‚ä½•å¤„ç†å›¾ç¥ç»ç½‘ç»œä¸­æ¯å±‚çš„ç‰¹å¾ã€‚
#if self.gnn.agg_modes[-1] == 'flatten'::

#è¿™ä¸ªæ¡ä»¶åˆ¤æ–­äº†å½“å‰çš„èšåˆæ¨¡å¼æ˜¯å¦ä¸º 'flatten'ã€‚å¦‚æœä¸º 'flatten'ï¼Œè¡¨ç¤ºå½“å‰å±‚çš„ç‰¹å¾ä¼šè¢«å±•å¹³ï¼ˆflattenedï¼‰ï¼Œå³å°†å¤šä¸ªå¤´çš„ç‰¹å¾è¿æ¥æˆä¸€ä¸ªé•¿çš„ç‰¹å¾å‘é‡ã€‚
#gnn_out_feats = self.gnn.hidden_feats[-1] * self.gnn.num_heads[-1]:

#å¦‚æœèšåˆæ¨¡å¼æ˜¯ 'flatten'ï¼Œåˆ™è¾“å‡ºç‰¹å¾ç»´åº¦ gnn_out_feats è¢«è®¾ç½®ä¸º self.gnn.hidden_feats[-1] å’Œ self.gnn.num_heads[-1] çš„ä¹˜ç§¯ã€‚
#self.gnn.hidden_feats[-1] æ˜¯å½“å‰å±‚çš„éšè—ç‰¹å¾ç»´åº¦ã€‚
#self.gnn.num_heads[-1] æ˜¯å½“å‰å±‚çš„æ³¨æ„åŠ›å¤´çš„æ•°é‡ã€‚
#è¿™é‡Œå‡è®¾æ¯ä¸ªæ³¨æ„åŠ›å¤´äº§ç”Ÿ self.gnn.hidden_feats[-1] ç»´çš„ç‰¹å¾ï¼Œæ‰€æœ‰å¤´çš„ç‰¹å¾è¢«å±•å¹³åæ€»ç»´åº¦ä¸º self.gnn.hidden_feats[-1] * self.gnn.num_heads[-1]ã€‚

    def forward(self, bg, feats):
        node_feats = self.gnn(bg, feats)
        return self.predict(node_feats)


# class ModifiedChargeModel(nn.Module):
#     def __init__(self,
#                  node_feat_size,
#                  edge_feat_size,
#                  num_layers=2,
#                  graph_feat_size=200,
#                  dropout=0.):
#         super(ModifiedChargeModel, self).__init__()
#
#         self.gnn = AttentiveFPGNN(node_feat_size=node_feat_size,
#                                   edge_feat_size=edge_feat_size,
#                                   num_layers=num_layers,
#                                   graph_feat_size=graph_feat_size,
#                                   dropout=dropout)
#         self.predict = nn.Sequential(
#             nn.Dropout(dropout),
#             nn.Linear(graph_feat_size, 1)
#         )
#
#     def forward(self, g, node_feats, edge_feats):
#         node_feats = self.gnn(g, node_feats, edge_feats)
#         return self.predict(node_feats)


# incorporate both the node and edge features using Multilayer Perception  ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœº
class AttentiveMLP1(nn.Module):
    def __init__(self, node_feat_size, edge_feat_size, edge_hidden_size, dropout):
        super(AttentiveMLP1, self).__init__()

        self.edge_transform = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(edge_feat_size, edge_hidden_size)
        )
        # self.gru = nn.GRUCell(edge_hidden_size, node_feat_size)
        self.MPL = nn.Sequential(
            nn.Linear(edge_hidden_size + node_feat_size, node_feat_size),
            nn.Dropout(dropout),
            nn.ReLU(),
            nn.Linear(node_feat_size, node_feat_size),
            nn.Dropout(dropout)
        )

    def forward(self, g, edge_logits, edge_feats, node_feats):
        g = g.local_var()
        g.edata['e'] = edge_softmax(g, edge_logits) * self.edge_transform(edge_feats)
        g.update_all(fn.copy_edge('e', 'm'), fn.sum('m', 'c'))
        context = F.elu(g.ndata['c'])
        # return F.relu(self.gru(context, node_feats))
        return F.relu(self.MPL(torch.cat([context, node_feats], dim=1)))


class AttentiveMLP2(nn.Module):
    def __init__(self, node_feat_size, edge_hidden_size, dropout):
        super(AttentiveMLP2, self).__init__()

        self.project_node = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(node_feat_size, edge_hidden_size)
        )
        # self.gru = nn.GRUCell(edge_hidden_size, node_feat_size)
        self.MPL = nn.Sequential(
            nn.Linear(edge_hidden_size + node_feat_size, node_feat_size),
            nn.Dropout(dropout),
            nn.ReLU(),
            nn.Linear(node_feat_size, node_feat_size),
            nn.Dropout(dropout)
        )

    def forward(self, g, edge_logits, node_feats):
        g = g.local_var()
        g.edata['a'] = edge_softmax(g, edge_logits)
        g.ndata['hv'] = self.project_node(node_feats)

        g.update_all(fn.src_mul_edge('hv', 'a', 'm'), fn.sum('m', 'c'))
        context = F.elu(g.ndata['c'])
        # return F.relu(self.gru(context, node_feats))
        return F.relu(self.MPL(torch.cat([context, node_feats], dim=1)))


class GetMLPContext(nn.Module):
    def __init__(self, node_feat_size, edge_feat_size, graph_feat_size, dropout):
        super(GetMLPContext, self).__init__()

        self.project_node = nn.Sequential(
            nn.Linear(node_feat_size, graph_feat_size),
            nn.LeakyReLU()
        )
        self.project_edge1 = nn.Sequential(
            nn.Linear(node_feat_size + edge_feat_size, graph_feat_size),
            nn.LeakyReLU()
        )
        self.project_edge2 = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(2 * graph_feat_size, 1),
            nn.LeakyReLU()
        )
        # self.attentive_gru = AttentiveGRU1(graph_feat_size, graph_feat_size,
        #                                    graph_feat_size, dropout)
        self.attentive_mlp = AttentiveMLP1(graph_feat_size, graph_feat_size,
                                           graph_feat_size, dropout)

    def apply_edges1(self, edges):
        return {'he1': torch.cat([edges.src['hv'], edges.data['he']], dim=1)}

    def apply_edges2(self, edges):
        return {'he2': torch.cat([edges.dst['hv_new'], edges.data['he1']], dim=1)}

    def forward(self, g, node_feats, edge_feats):
        g = g.local_var()
        g.ndata['hv'] = node_feats
        g.ndata['hv_new'] = self.project_node(node_feats)
        g.edata['he'] = edge_feats

        g.apply_edges(self.apply_edges1)
        g.edata['he1'] = self.project_edge1(g.edata['he1'])
        g.apply_edges(self.apply_edges2)
        logits = self.project_edge2(g.edata['he2'])

        return self.attentive_mlp(g, logits, g.edata['he1'], g.ndata['hv_new'])


class GNNMLPLayer(nn.Module):
    def __init__(self, node_feat_size, graph_feat_size, dropout):
        super(GNNMLPLayer, self).__init__()

        self.project_edge = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(2 * node_feat_size, 1),
            nn.LeakyReLU()
        )
        # self.attentive_gru = AttentiveGRU2(node_feat_size, graph_feat_size, dropout)
        self.attentive_mlp = AttentiveMLP2(node_feat_size, graph_feat_size, dropout)
        self.bn_layer = nn.BatchNorm1d(graph_feat_size)

    def apply_edges(self, edges):
        return {'he': torch.cat([edges.dst['hv'], edges.src['hv']], dim=1)}

    def forward(self, g, node_feats):
        g = g.local_var()
        g.ndata['hv'] = node_feats
        g.apply_edges(self.apply_edges)
        logits = self.project_edge(g.edata['he'])

        # return self.bn_layer(self.attentive_gru(g, logits, node_feats))
        return self.bn_layer(self.attentive_mlp(g, logits, node_feats))


class GNNMLP(nn.Module):
    def __init__(self,
                 node_feat_size,
                 edge_feat_size,
                 num_layers=2,
                 graph_feat_size=200,
                 dropout=0.):
        super(GNNMLP, self).__init__()

        self.init_context = GetMLPContext(node_feat_size, edge_feat_size, graph_feat_size, dropout)
        self.gnn_layers = nn.ModuleList()
        self.sum_node_feats = 0
        for _ in range(num_layers - 1):
            self.gnn_layers.append(GNNMLPLayer(graph_feat_size, graph_feat_size, dropout))

    def forward(self, g, node_feats, edge_feats):
        node_feats = self.init_context(g, node_feats, edge_feats)
        self.sum_node_feats = node_feats
        for gnn in self.gnn_layers:
            node_feats = gnn(g, node_feats)
            self.sum_node_feats = self.sum_node_feats + node_feats
        return self.sum_node_feats


class GNNMLPPredictor(nn.Module):
    def __init__(self,
                 node_feat_size,
                 edge_feat_size,
                 num_layers=2,
                 graph_feat_size=200,
                 dropout=0.):
        super(GNNMLPPredictor, self).__init__()

        self.gnn = GNNMLP(node_feat_size=node_feat_size,
                          edge_feat_size=edge_feat_size,
                          num_layers=num_layers,
                          graph_feat_size=graph_feat_size,
                          dropout=dropout)
        self.predict = nn.Sequential(
            nn.Linear(graph_feat_size, graph_feat_size),
            nn.Dropout(dropout),
            nn.ReLU(),
            nn.Linear(graph_feat_size, 1)
        )

    def forward(self, g, node_feats, edge_feats):
        sum_node_feats = self.gnn(g, node_feats, edge_feats)
        return self.predict(sum_node_feats)
